name: Sync Google Sheet to JSON

on:
  schedule:
    - cron: "0 * * * *"        # 每小時
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      # 👉 換成你 Google Sheet「發佈為 CSV」嘅 URL（注意 gid）
      SHEET_URL: "https://docs.google.com/spreadsheets/d/e/2PACX-1vSpt865HzuchDUxgZIPL3RBbZ-VneXtRFgM0Gt0fDAla2yk_Afh4t5iw-6a2U0lmqSiiQagcSE6Rjk9/pub?output=csv"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas

      - name: Fetch Google Sheet CSV
        run: |
          set -e
          CODE=$(curl -sSL -w "%{http_code}" -o sheet.csv "$SHEET_URL")
          echo "HTTP_STATUS=$CODE"
          if [ "$CODE" -ne 200 ]; then
            echo "Download failed ($CODE)" >&2
            exit 1
          fi
          echo "==== sheet.csv meta ===="
          wc -l sheet.csv || true
          echo "---- head ----"; head -n 5 sheet.csv || true

      - name: Convert CSV to JSON (new schema with zh/en names & tags)
        run: |
          python - <<'PY'
import pandas as pd, json, re, sys

# ---------- helpers ----------
def norm_str(v):
    if v is None: return None
    s = str(v).strip()
    return s if s else None

def split_list(v):
    s = norm_str(v)
    if not s: return []
    s = s.replace('；',';').replace('，',',')
    for sp in ['|',';','/','、','／']:
        s = s.replace(sp, ',')
    return [p.strip() for p in s.split(',') if p.strip()]

def norm_states(v):
    return [x.upper() for x in split_list(v)]

def to_float(x):
    try:
        return float(x) if x is not None and str(x).strip() != '' else None
    except Exception:
        return None

def norm_url(v):
    s = norm_str(v)
    if not s: return None
    if not re.match(r'^[a-zA-Z]+://', s):
        s = 'https://' + s
    return s

# ---------- load csv ----------
try:
    df = pd.read_csv('sheet.csv', dtype=str, keep_default_na=False)
except Exception as e:
    print('CSV read error:', e); sys.exit(1)

# 建立「大小寫不敏感 + 去空白」嘅欄位映射
colmap = {c.strip().lower(): c for c in df.columns}

def get_col(*candidates):
    """按候選名（已包含中英/底線/大小寫變體）從 df 取 Series；搵唔到就回空字串 Series"""
    for cand in candidates:
        key = cand.strip().lower()
        if key in colmap: 
            return df[colmap[key]]
    return pd.Series(['']*len(df))

# 可接受嘅表頭（覆蓋英文/底線/中文）
col_name_zh = get_col('namezh','name_zh','中文名','店名中文','name (zh)','name zh')
col_name_en = get_col('nameen','name_en','英文名','店名英文','name (en)','name en')
col_name_legacy = get_col('name')

col_tags_zh = get_col('tagszh','tags_zh','中文tag','標籤中文','tags (zh)','tags zh')
col_tags_en = get_col('tagsen','tags_en','英文tag','標籤英文','tags (en)','tags en')
col_tags_legacy = get_col('tags')

col_suburb = get_col('suburb','area','地區','區')   # suburb 優先，否則兼容 area / 地區
col_states = get_col('states','state','州','省')
col_cities = get_col('cities','city','城市','市')

col_chainId = get_col('chainid','chain_id','chainId')
col_branchName = get_col('branchname','branch_name','branchName')

col_category = get_col('category','分類','類別')
col_address  = get_col('address','地址')
col_email    = get_col('email')
col_phone    = get_col('phone','電話','聯絡電話')
col_website  = get_col('website','網址')

col_lat = get_col('lat','緯度')
col_lng = get_col('lng','經度')

col_isSponsored = get_col('isSponsored','is_sponsored','sponsored','贊助','是否贊助')
col_sponsorTier = get_col('sponsorTier','sponsor_tier','贊助級別')
col_sponsorStart= get_col('sponsorStart','sponsor_start','贊助開始')
col_sponsorEnd  = get_col('sponsorEnd','sponsor_end','贊助結束')
col_adLink      = get_col('adLink','ad_link','廣告連結')
col_adNote      = get_col('adNote','ad_note','廣告備註')

out = []
for i in range(len(df)):
    # ---- names ----
    name_legacy = norm_str(col_name_legacy.iloc[i])
    nameZh = norm_str(col_name_zh.iloc[i])
    nameEn = norm_str(col_name_en.iloc[i])

    # 如果只有中文而中文格式似「中文 (English)」→ 自動拆英文（保險網）
    if (not nameEn) and nameZh:
        m = re.match(r'^\s*(.+?)\s*[\(（]\s*([A-Za-z0-9 .,\'+&/-]+)\s*[\)）]\s*$', nameZh)
        if m:
            nameZh = m.group(1).strip()
            nameEn = m.group(2).strip()

    # 至少要有任一名稱
    if not (name_legacy or nameZh or nameEn):
        continue

    # ---- base item ----
    item = {
        # 兼容舊 App：仍寫入 legacy name（無舊就用英文，最後中文）
        'name'     : name_legacy or (nameEn or nameZh or ''),
        'nameZh'   : nameZh,
        'nameEn'   : nameEn,

        'category' : norm_str(col_category.iloc[i]) or 'uncategorized',

        # tags：分中英；英文搵唔到時以舊 tags 補位
        'tagsZh'   : split_list(col_tags_zh.iloc[i]),
        'tagsEn'   : (split_list(col_tags_en.iloc[i]) or split_list(col_tags_legacy.iloc[i])),

        # suburb / area
        'suburb'   : norm_str(col_suburb.iloc[i]) or None,
        'area'     : norm_str(get_col('area','地區').iloc[i]) or None,

        'address'  : norm_str(col_address.iloc[i]),
        'email'    : norm_str(col_email.iloc[i]),
        'phone'    : norm_str(col_phone.iloc[i]),
        'website'  : norm_url(col_website.iloc[i]),

        'chainId'    : norm_str(col_chainId.iloc[i]),
        'branchName' : norm_str(col_branchName.iloc[i]),
    }

    # states / cities
    states = norm_states(col_states.iloc[i])
    item['states'] = states
    cities = split_list(col_cities.iloc[i])
    if cities: item['cities'] = cities

    # lat/lng
    lat = norm_str(col_lat.iloc[i]); lng = norm_str(col_lng.iloc[i])
    if lat is not None or lng is not None:
        item['lat'] = to_float(lat)
        item['lng'] = to_float(lng)

    # sponsor / ad
    isSponsored = norm_str(col_isSponsored.iloc[i])
    if isSponsored is not None:
        item['isSponsored'] = isSponsored.lower() in ('1','true','yes','y','是','有')
    for (k, series) in [
        ('sponsorTier',  col_sponsorTier),
        ('sponsorStart', col_sponsorStart),
        ('sponsorEnd',   col_sponsorEnd),
        ('adLink',       col_adLink),
        ('adNote',       col_adNote),
    ]:
        v = norm_str(series.iloc[i])
        if k == 'adLink': v = norm_url(v)
        if v is not None: item[k] = v

    # 舊 tags（完全保留，俾 App 舊欄位兼容）
    legacy_tags = split_list(col_tags_legacy.iloc[i])
    if legacy_tags: item['tags'] = legacy_tags

    out.append(item)

with open('businesses.json','w',encoding='utf-8') as f:
    json.dump(out, f, ensure_ascii=False, indent=2, sort_keys=True)

print('Wrote businesses.json with', len(out), 'items')
