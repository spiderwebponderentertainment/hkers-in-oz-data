name: Sync Google Sheet to JSON

on:
  schedule:
    - cron: "0 * * * *"   # 每小時跑一次
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      SHEET_URL: "https://docs.google.com/spreadsheets/d/1kHUqAVX7t_HJ4RbKsvbAryeFc3d9XWj_Zw7iU8h4Jfw/export?format=csv&gid=0"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas

      - name: Fetch Google Sheet CSV
        run: |
          set -e
          echo "→ Try: $SHEET_URL"
          CODE=$(curl -sSL -w "%{http_code}" -o sheet.csv "$SHEET_URL")
          echo "HTTP_STATUS=$CODE"
          if [ "$CODE" -ne 200 ]; then
            echo "Download failed ($CODE) — first 500 bytes of response:" >&2
            head -c 500 sheet.csv >&2 || true
            exit 1
          fi
          echo "==== sheet.csv meta ===="
          wc -l sheet.csv || true
          echo "---- head ----"; head -n 2 sheet.csv || true

      - name: Convert CSV to JSON (with HK fields, robust headers)
        run: |
          python - <<'PY'
          import pandas as pd, json, re, sys

          def norm_header(h: str) -> str:
              # 將 NBSP 變普通空格、trim、轉小寫
              return str(h).replace('\u00A0',' ').strip().lower()

          def norm_str(v):
              if v is None: return None
              s = str(v).replace('\u00A0',' ').strip()
              return s if s else None

          def split_list(v):
              s = norm_str(v)
              if not s: return []
              s = s.replace('；',';').replace('，',',')
              for sp in ['|',';','/','、','／']:
                  s = s.replace(sp, ',')
              return [p.strip() for p in s.split(',') if p.strip()]

          def norm_states(v):
              return [x.upper() for x in split_list(v)]

          def to_float(x):
              try:
                  return float(x) if x is not None and str(x).strip() != '' else None
              except Exception:
                  return None

          def norm_url(v):
              s = norm_str(v)
              if not s: return None
              if not re.match(r'^[a-zA-Z]+://', s):
                  s = 'https://' + s
              return s

          try:
              df = pd.read_csv('sheet.csv', dtype=str, keep_default_na=False, encoding='utf-8')
          except Exception as e:
              print('CSV read error:', e); sys.exit(1)

          # 建立「正規化後表頭」→「原表頭」對照
          colmap = {norm_header(c): c for c in df.columns}
          print("== Debug: normalized headers ==")
          for k,v in colmap.items():
              print(f"  {k!r}  <-  {v!r}")

          def get_col(*cands):
              for cand in cands:
                  key = norm_header(cand)
                  if key in colmap:
                      return df[colmap[key]]
              # 找唔到就回一列空白，並打印提示
              print("[WARN] missing column among:", cands, file=sys.stderr)
              return pd.Series(['']*len(df))

          # 基本欄位
          col_name_zh = get_col('namezh','name_zh','中文名')
          col_name_en = get_col('nameen','name_en','英文名')
          col_tags_zh = get_col('tagszh','tags_zh','標籤_中')
          col_tags_en = get_col('tagsen','tags_en','標籤_英')
          col_tags_legacy = get_col('tags','標籤')
          col_suburb = get_col('suburb')
          col_states = get_col('states','state')
          col_cities = get_col('cities','city')
          col_chainId = get_col('chainid','chain_id')
          col_branchName = get_col('branchname','branch_name')
          col_category = get_col('category')
          col_address  = get_col('address')
          col_email    = get_col('email')
          col_phone    = get_col('phone')
          col_website  = get_col('website')
          col_lat = get_col('lat')
          col_lng = get_col('lng')

          # sponsor / ad
          col_isSponsored = get_col('issponsored','is_sponsored')
          col_sponsorTier = get_col('sponsortier','sponsor_tier')
          col_sponsorStart= get_col('sponsorstart','sponsor_start')
          col_sponsorEnd  = get_col('sponsorend','sponsor_end')
          col_adLink      = get_col('adlink','ad_link')
          col_adNote      = get_col('adnote','ad_note')

          # 🔥 HK owner / features（加入備選名、大小寫兼容）
          col_hkOwnerZh = get_col('hkownerstatuszh','hk_owner_status_zh','ownerstatuszh')
          col_hkOwnerEn = get_col('hkownerstatusen','hk_owner_status_en','ownerstatusen')
          col_hkFeatZh  = get_col('hkfeatureszh','hk_features_zh','hkfeaturezh','hk_feature_zh')
          col_hkFeatEn  = get_col('hkfeaturesen','hk_features_en','hkfeatureen','hk_feature_en','hkfeaturesen ')  # 尾空格也兼容

          out = []
          for i in range(len(df)):
              nameZh = norm_str(col_name_zh.iloc[i])
              nameEn = norm_str(col_name_en.iloc[i])
              if not (nameZh or nameEn): 
                  continue

              item = {
                'nameZh'   : nameZh,
                'nameEn'   : nameEn,
                'category' : norm_str(col_category.iloc[i]) or 'uncategorized',
                'tagsZh'   : split_list(col_tags_zh.iloc[i]),
                'tagsEn'   : (split_list(col_tags_en.iloc[i]) or split_list(col_tags_legacy.iloc[i])),
                'suburb'   : norm_str(col_suburb.iloc[i]) or None,
                'address'  : norm_str(col_address.iloc[i]),
                'email'    : norm_str(col_email.iloc[i]),
                'phone'    : norm_str(col_phone.iloc[i]),
                'website'  : norm_url(col_website.iloc[i]),
                'chainId'    : norm_str(col_chainId.iloc[i]),
                'branchName' : norm_str(col_branchName.iloc[i]),
              }

              # states / cities
              item['states'] = norm_states(col_states.iloc[i])
              cities = split_list(col_cities.iloc[i])
              if cities: item['cities'] = cities

              # lat/lng
              lat = norm_str(col_lat.iloc[i]); lng = norm_str(col_lng.iloc[i])
              if lat or lng:
                  item['lat'] = to_float(lat)
                  item['lng'] = to_float(lng)

              # sponsor
              isSponsored = norm_str(col_isSponsored.iloc[i])
              if isSponsored is not None:
                  item['isSponsored'] = isSponsored.lower() in ('1','true','yes','y','是','有')
              for (k, series) in [
                  ('sponsorTier',  col_sponsorTier),
                  ('sponsorStart', col_sponsorStart),
                  ('sponsorEnd',   col_sponsorEnd),
                  ('adLink',       col_adLink),
                  ('adNote',       col_adNote),
              ]:
                  v = norm_str(series.iloc[i])
                  if k == 'adLink': v = norm_url(v)
                  if v is not None: item[k] = v

              # ✅ 香港相關（只在有值時輸出）
              hkOwnerZh = norm_str(col_hkOwnerZh.iloc[i])
              hkOwnerEn = norm_str(col_hkOwnerEn.iloc[i])
              if hkOwnerZh: item['hkOwnerStatusZh'] = hkOwnerZh
              if hkOwnerEn: item['hkOwnerStatusEn'] = hkOwnerEn
              featsZh = split_list(col_hkFeatZh.iloc[i])
              featsEn = split_list(col_hkFeatEn.iloc[i])
              if featsZh: item['hkFeaturesZh'] = featsZh
              if featsEn: item['hkFeaturesEn'] = featsEn

              out.append(item)

          with open('businesses.json','w',encoding='utf-8') as f:
              json.dump(out, f, ensure_ascii=False, indent=2, sort_keys=True)

          print('Wrote businesses.json with', len(out), 'items')
          # 額外：數下幾多項有 hkFeaturesEn
          cnt_en = sum(1 for it in out if 'hkFeaturesEn' in it)
          print('Items with hkFeaturesEn:', cnt_en)
          PY

      - name: Validate output
        run: |
          echo "---- keys check (first 3 items) ----"
          python - <<'PY'
          import json
          data = json.load(open('businesses.json'))
          for i, it in enumerate(data[:3]):
              print(i, sorted(it.keys()))
          bad = [i for i,it in enumerate(data) if 'name' in it]
          print("items with legacy 'name' key:", len(bad))
          have_en = [i for i,it in enumerate(data) if 'hkFeaturesEn' in it]
          print("items with hkFeaturesEn:", len(have_en))
          PY

      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v6
        with:
          commit-message: "chore: sync businesses.json from Google Sheet"
          title: "chore: sync businesses.json from Google Sheet"
          body: |
            Auto-generated by **Sync Google Sheet to JSON** workflow.
            - Source: Google Sheet (published CSV)
            - Output: `businesses.json`
          branch: automation/sheet-sync
          token: ${{ secrets.GITHUB_TOKEN }}
          delete-branch: true
          add-paths: |
            businesses.json
