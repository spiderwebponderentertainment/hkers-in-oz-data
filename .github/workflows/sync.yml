name: Sync Google Sheet to JSON

on:
  schedule:
    - cron: "0 * * * *"           # æ¯å°æ™‚
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      # ðŸ‘‰ æ›æˆä½  Google Sheetã€Œç™¼ä½ˆç‚º CSVã€å˜… URLï¼ˆæ³¨æ„ gidï¼‰
      SHEET_URL: "https://docs.google.com/spreadsheets/d/e/2PACX-XXX/pub?gid=0&single=true&output=csv"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas

      - name: Fetch Google Sheet CSV
        run: |
          set -e
          CODE=$(curl -sSL -w "%{http_code}" -o sheet.csv "$SHEET_URL")
          echo "HTTP_STATUS=$CODE"
          if [ "$CODE" -ne 200 ]; then
            echo "Download failed ($CODE)" >&2
            exit 1
          fi
          echo "==== sheet.csv meta ===="
          wc -l sheet.csv || true
          echo "---- head ----"; head -n 5 sheet.csv || true

      - name: Convert CSV to JSON (new schema with zh/en names & tags)
        run: |
          python - <<'PY'
          import pandas as pd, json, re, sys

          # --- helpers ---
          def norm_str(v):
              if v is None: return None
              s = str(v).strip()
              return s if s else None

          def split_list(v):
              s = norm_str(v)
              if not s: return []
              s = s.replace('ï¼›',';').replace('ï¼Œ',',')
              for sp in ['|',';','/','ã€']:
                  s = s.replace(sp, ',')
              return [p.strip() for p in s.split(',') if p.strip()]

          def norm_states(v):
              return [x.upper() for x in split_list(v)]

          def norm_url(v):
              s = norm_str(v)
              if not s: return None
              if not re.match(r'^[a-zA-Z]+://', s):
                  s = 'https://' + s
              return s

          def to_float(x):
              try:
                  return float(x) if x is not None and str(x).strip() != '' else None
              except Exception:
                  return None

          # --- load csv as strings ---
          try:
              df = pd.read_csv('sheet.csv', dtype=str, keep_default_na=False)
          except Exception as e:
              print('CSV read error:', e); sys.exit(1)

          if len(df) == 0:
              print('No rows in CSV, abort.'); sys.exit(1)

          # normalise columns to avoid KeyError
          cols = {c.lower(): c for c in df.columns}  # case-insensitive map
          def get(colname):
              key = colname.lower()
              if key in cols:
                  return df[cols[key]]
              return pd.Series(['']*len(df))

          out = []
          for _, r in df.iterrows():
              # --- names (new: nameZh/nameEn; fallback: name) ---
              nameZh = norm_str(r.get('nameZh') or r.get('namezh'))
              nameEn = norm_str(r.get('nameEn') or r.get('nameen'))
              legacyName = norm_str(r.get('name'))

              # è‡³å°‘è¦æœ‰ä¸€å€‹åå…ˆæ”¶
              if not (nameZh or nameEn or legacyName):
                  continue

              # --- basic fields ---
              item = {
                  'chainId'      : norm_str(r.get('chainId') or r.get('chainid')),
                  'branchName'   : norm_str(r.get('branchName') or r.get('branchname')),
                  'category'     : norm_str(r.get('category')) or 'uncategorized',
                  'states'       : norm_states(r.get('states')),
                  'address'      : norm_str(r.get('address')),
                  'email'        : norm_str(r.get('email')),
                  'phone'        : norm_str(r.get('phone')),
                  'website'      : norm_url(r.get('website')),
                  # æ–°æ¬„ä½
                  'nameZh'       : nameZh if nameZh else None,
                  'nameEn'       : nameEn if nameEn else (legacyName or None),
                  'suburb'       : norm_str(r.get('suburb')) or norm_str(r.get('area')),  # å…¼å®¹èˆŠ area
                  # tags zh/enï¼ˆå…¼å®¹èˆŠ tagsï¼‰
                  'tagsZh'       : split_list(r.get('tagsZh') or r.get('tagszh')),
                  'tagsEn'       : split_list(r.get('tagsEn') or r.get('tagsen') or r.get('tags')),
              }

              # states å–®å€¼å¾Œå‚™ï¼ˆèˆŠ sheet å¯èƒ½æœ‰ stateï¼‰
              state = norm_str(r.get('state'))
              if not item['states'] and state:
                  item['states'] = [state.upper()]

              # citiesï¼ˆå…¼å®¹å–®å€¼ cityï¼‰
              cities = split_list(r.get('cities'))
              city   = norm_str(r.get('city'))
              if cities: item['cities'] = cities
              elif city: item['cities'] = [city]

              # lat/lng
              lat = norm_str(r.get('lat')); lng = norm_str(r.get('lng'))
              if lat is not None or lng is not None:
                  item['lat'] = to_float(lat)
                  item['lng'] = to_float(lng)

              # sponsor / ads
              isSponsored = norm_str(r.get('isSponsored') or r.get('issponsored'))
              if isSponsored is not None:
                  item['isSponsored'] = isSponsored.lower() in ('1','true','yes','y')
              for k in ['sponsorTier','sponsorStart','sponsorEnd','adLink','adNote']:
                  v = norm_str(r.get(k) or r.get(k.lower()))
                  if k == 'adLink': v = norm_url(v)
                  if v is not None: item[k] = v

              out.append(item)

          with open('businesses.json','w',encoding='utf-8') as f:
              json.dump(out, f, ensure_ascii=False, indent=2, sort_keys=True)

          print('Wrote businesses.json with', len(out), 'items')
          PY

      - name: Show diff
        run: |
          echo "==== git status ===="
          git status --porcelain
          echo "==== diff (businesses.json) ===="
          git --no-pager diff -- businesses.json || true

      - name: Create Pull Request
        id: cpr
        uses: peter-evans/create-pull-request@v6
        with:
          commit-message: "chore: sync businesses.json from Google Sheet"
          title: "chore: sync businesses.json from Google Sheet"
          body: |
            Auto-generated by **Sync Google Sheet to JSON** workflow.
            - Source: Google Sheet (published CSV)
            - Output: `businesses.json`
          branch: automation/sheet-sync
          token: ${{ secrets.GITHUB_TOKEN }}
          delete-branch: true
          add-paths: |
            businesses.json

      - name: Enable auto-merge on PR
        if: ${{ steps.cpr.outputs['pull-request-number'] }}
        uses: peter-evans/enable-pull-request-automerge@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          pull-request-number: ${{ steps.cpr.outputs['pull-request-number'] }}
          merge-method: squash

      - name: Merge PR via API (best effort)
        if: ${{ steps.cpr.outputs['pull-request-number'] }}
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const pr = Number(core.getInput('pr') || '${{ steps.cpr.outputs["pull-request-number"] }}');
            try {
              const { data } = await github.rest.pulls.merge({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: pr,
                merge_method: 'squash'
              });
              core.info(`Merged PR #${pr}: ${data.sha}`);
            } catch (e) {
              core.warning(`Merge failed (branch protection maybe). ${e.message}`);
            }
          result-encoding: string
          with:
            pr: ${{ steps.cpr.outputs['pull-request-number'] }}
