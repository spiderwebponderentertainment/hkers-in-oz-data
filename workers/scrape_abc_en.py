# workers/scrape_abc_en.py
# -*- coding: utf-8 -*-
import re, sys, json, time, hashlib
from datetime import datetime, timezone
from urllib.parse import urljoin, urlencode
import requests
from bs4 import BeautifulSoup

# ---------------- åŸºæœ¬è¨­å®š ----------------
HEADERS = {
    # é è¨­ç”¨ Desktop Chromeï¼ˆè¼ƒå°‘è¢«æ“‹ï¼‰
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-AU,en;q=0.9",
    "Referer": "https://www.google.com/",
}
REQ_TIMEOUT = (8, 20)   # (connect, read)
MAX_CRAWL   = 250       # æœ€å¤šå˜—è©¦æŠ“å–çš„æ–‡ç«  URL æ•¸
MAX_OUTPUT  = 150       # è¼¸å‡ºæ•¸é‡ä¸Šé™ï¼ˆæœ€æ–°å„ªå…ˆï¼‰

session = requests.Session()
session.headers.update(HEADERS)

# åªå…è¨±ã€Œæ­£æ–‡ã€é é¢ï¼š/news/YYYY-MM-DD/.../<numeric-id>
GOOD_URL_RE = re.compile(
    r"^https?://www\.abc\.net\.au/news/\d{4}-\d{2}-\d{2}/[^?#]+/\d+/?$"
)

# å·²çŸ¥ä¸éœ€è¦æˆ–å®¹æ˜“ 403/404 çš„å‰ç¶´
BAD_PREFIXES = (
    "https://www.abc.net.au/news/topic/",
    "https://www.abc.net.au/news/subscribe",
    "https://www.abc.net.au/news/emergency",
    "https://www.abc.net.au/news/chinese",
    "https://www.abc.net.au/news/indonesian",
    "https://www.abc.net.au/news/tok-pisin",
)

# ä¸Ÿæ£„å«æœ‰ä»¥ä¸‹é—œéµå­—çš„éˆæ¥
BAD_KEYWORDS = ("live-blog", "/page/", "#")

# ï¼ˆå¯é¸ï¼‰ä¿åº•ç™½åå–®ï¼šé‡åˆ°æƒ³å¿…æ”¶éŒ„çš„ URL å¯æ”¾å…¥
ALWAYS_INCLUDE = set()

# ---------------- ç¨®å­é ï¼ˆä¸åšåˆ†é ï¼‰ ----------------
SEED_PAGES = [
    "https://www.abc.net.au/news/",
    "https://www.abc.net.au/news/justin/",
    "https://www.abc.net.au/news/politics/",
    "https://www.abc.net.au/news/world/",
    "https://www.abc.net.au/news/business/",
    "https://www.abc.net.au/news/sport/",
    "https://www.abc.net.au/news/health/",
    "https://www.abc.net.au/news/science/",
    "https://www.abc.net.au/news/environment/",
]

# Google Newsï¼ˆEnglish + AUï¼‰ä½œè£œä½
def _google_news_rss_url():
    base = "https://news.google.com/rss/search"
    qs = {
        "q": "site:abc.net.au/news",
        "hl": "en-AU",
        "gl": "AU",
        "ceid": "AU:en",
    }
    return f"{base}?{urlencode(qs)}"


# ---------------- HTTP Helpers ----------------
# å–ä»£åŸæœ¬çš„ _normalize_https
def _normalize_https(u: str) -> str:
    # å¼·åˆ¶ httpsï¼Œå»é™¤ fragment + queryï¼ˆutm ç­‰ï¼‰
    u = u.replace("http://", "https://")
    u = u.split("#", 1)[0]
    u = u.split("?", 1)[0]   # ğŸ‘ˆ æ–°å¢ï¼šç§»é™¤ ?utm=... ç­‰ querystring
    return u

def get(url, **kw):
    """
    å¸¶ UA/AMP è®Šé«” fallback çš„ GETã€‚
    - å…ˆ Desktop UA å† Mobile UAã€‚
    - è©¦æ™®é€šé ã€?ampã€/?output=ampã€/?outputType=ampã€‚
    """
    url = _normalize_https(url)
    variants = [url]
    if not url.endswith("?amp"):
        variants.append(url + ("&amp" if "?" in url else "?amp"))
    base = url.rstrip("/")
    if not base.endswith("/output=amp"):
        variants.append(base + "/?output=amp")
    if not base.endswith("/outputType=amp"):
        variants.append(base + "/?outputType=amp")

    ua_desktop = HEADERS["User-Agent"]
    ua_mobile  = ("Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) "
                  "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 "
                  "Mobile/15E148 Safari/604.1")

    last_exc = None
    for ua in (ua_desktop, ua_mobile):
        session.headers["User-Agent"] = ua
        for v in variants:
            for attempt in range(2):
                try:
                    r = session.get(v, timeout=REQ_TIMEOUT, **kw)
                    if r.status_code in (403, 406):
                        time.sleep(0.3)
                        continue
                    r.raise_for_status()
                    return r
                except requests.RequestException as e:
                    last_exc = e
                    time.sleep(0.4 * (attempt + 1))
    if last_exc:
        raise last_exc
    raise requests.RequestException("Unknown fetch error")

# ---------------- Link æŠ½å–èˆ‡ç¯©é¸ ----------------
def _looks_like_article(u: str) -> bool:
    if any(k in u for k in BAD_KEYWORDS):
        return False
    if u.startswith(BAD_PREFIXES):
        return False
    return bool(GOOD_URL_RE.match(u))

def extract_links(html_text, base):
    soup = BeautifulSoup(html_text, "lxml")
    urls = set()
    for a in soup.select("a[href]"):
        href = a["href"].strip()
        u = urljoin(base, href)
        u = _normalize_https(u)
        if _looks_like_article(u):
            urls.add(u)
    return urls

def _id_from_url(u: str) -> int:
    m = re.search(r"/(\d+)/?$", u)
    return int(m.group(1)) if m else -1

def _date_from_url(u: str):
    m = re.search(r"/news/(\d{4}-\d{2}-\d{2})/", u)
    if not m:
        return None
    try:
        dt = datetime.fromisoformat(m.group(1) + "T00:00:00+00:00")
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

# ---------------- æ–‡ç« è§£æ ----------------
def _text(x):
    if not x:
        return ""
    return " ".join(x.get_text(" ", strip=True).split())

def _first(soup, selectors):
    for sel in selectors:
        el = soup.select_one(sel)
        if el:
            return el
    return None

def _parse_datetime(s: str):
    try:
        dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
        return dt.astimezone(timezone.utc)
    except Exception:
        return None

def parse_article(url: str, html_text: str):
    """
    çµ±ä¸€çµæ§‹ï¼š
      url, title, published_at(UTC ISO8601|None), content, source="abc_en", id
    """
    soup = BeautifulSoup(html_text, "lxml")

    # æ¨™é¡Œï¼šog:title / <meta name="title"> / <h1>
    title = ""
    ogt = soup.select_one('meta[property="og:title"]')
    if ogt and ogt.get("content"):
        title = ogt["content"].strip()
    if not title:
        mt = soup.select_one('meta[name="title"]')
        if mt and mt.get("content"):
            title = mt["content"].strip()
    if not title:
        h1 = _first(soup, ["h1", "header h1", "article h1"])
        title = _text(h1) if h1 else ""

    # ç™¼ä½ˆæ™‚é–“ï¼šå¤šå€‹å€™é¸ + <time>
    published = None
    for sel in [
        'meta[property="article:published_time"]',
        'meta[name="article:published_time"]',
        'meta[property="og:article:published_time"]',
        'meta[property="og:updated_time"]',  # æœ‰äº›ç¨¿åªæ›´æ–°å‘¢å€‹
    ]:
        m = soup.select_one(sel)
        if m and m.get("content"):
            published = _parse_datetime(m["content"].strip())
            if published:
                break
    if not published:
        t = soup.select_one("time[datetime]")
        if t and t.get("datetime"):
            published = _parse_datetime(t["datetime"].strip())
    if not published:
        published = _date_from_url(url)

    # å…§å®¹ï¼šæ­£æ–‡å®¹å™¨æŠ½ <p>ï¼›å…œåº•å…¨é  p
    body = ""
    body_root = _first(
        soup,
        [
            "article",
            '[data-component="article-body"]',
            ".article",
            ".story",
            ".article-body",
            "main",
        ],
    )
    if body_root:
        paras = [p for p in body_root.select("p") if _text(p)]
        body = "\n".join(_text(p) for p in paras)
    if not body:
        paras = [p for p in soup.select("p") if _text(p)]
        body = "\n".join(_text(p) for p in paras[:50])

    # è‡³å°‘è¦æœ‰ æ¨™é¡Œ æˆ– å…§å®¹
    if not title and not body:
        return None

    # ç©©å®š idï¼šå°¾æ®µæ•¸å­—ï¼Œå¦å‰‡ MD5(url)
    m = re.search(r"/(\d+)/?$", url)
    stable_id = m.group(1) if m else hashlib.md5(url.encode("utf-8")).hexdigest()

    return {
        "url": url,
        "title": title,
        "published_at": published.isoformat().replace("+00:00", "Z") if published else None,
        "content": body,
        "source": "abc_en",
        "id": stable_id,
    }

# ---------------- æŠ“å–ä¸»æµç¨‹ ----------------
def fetch_article(url):
    try:
        resp = get(url)
        resp.raise_for_status()
    except Exception as e:
        print(f"[WARN] fetch article fail {url}: {e}")
        return None
    item = parse_article(url, resp.text)
    if not item:
        print(f"[WARN] parse article empty {url}")
    return item

def _discover_from_seed():
    print(f"[INFO] entry bases: {len(SEED_PAGES)}")
    discovered = set()
    for seed in SEED_PAGES:
        try:
            r = get(seed)
            r.raise_for_status()
            links = extract_links(r.text, seed)
            discovered |= links
        except Exception as e:
            print(f"[WARN] entry scrape fail {seed}: {e}")
    return discovered

def _discover_from_google_news():
    urls = set()
    try:
        rss_url = _google_news_rss_url()
        r = get(rss_url)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "xml")
        for it in soup.select("item > link"):
            u = _normalize_https(it.text.strip())
            if _looks_like_article(u):
                urls.add(u)
    except Exception as e:
        print(f"[WARN] google news fetch fail: {e}")
    return urls

def crawl():
    # å…¥å£ï¼šSeed Pages + Google News + ç™½åå–®
    entry_urls = set()
    entry_urls |= _discover_from_seed()
    entry_urls |= _discover_from_google_news()
    entry_urls |= { _normalize_https(u) for u in ALWAYS_INCLUDE }

    # ç”¨å°¾æ®µæ•¸å­— ID å€’åºï¼ˆæœ€æ–°å„ªå…ˆï¼‰ï¼Œå†è£ MAX_CRAWL
    entry_urls = sorted(entry_urls, key=_id_from_url, reverse=True)
    print(f"[INFO] entry page urls: {len(entry_urls)}")

    entry_urls = entry_urls[:MAX_CRAWL]
    print(f"[INFO] crawl urls: {len(entry_urls)}")

    out = []
    for u in entry_urls:
        art = fetch_article(u)
        if art:
            out.append(art)
    return out

def _published_key(x):
    """
    æ’åºéµï¼šå„ªå…ˆç”¨ published_atï¼›å†‡å˜…è©±ç”¨ URL è£é¢å˜… YYYY-MM-DD ä½œå›é€€ï¼ˆè¨­ 12:00Zï¼‰ã€‚
    """
    ts = x.get("published_at")
    if ts:
        try:
            return int(datetime.fromisoformat(ts.replace("Z", "+00:00")).timestamp())
        except Exception:
            pass
    url = x.get("url", "")
    m = re.search(r"/(\d{4})-(\d{2})-(\d{2})/", url)
    if m:
        y, mo, d = map(int, m.groups())
        try:
            dt = datetime(y, mo, d, 12, 0, 0, tzinfo=timezone.utc)
            return int(dt.timestamp())
        except Exception:
            pass
    return 0

def main():
    items = crawl()

    # ç”¨ id å»é‡
    uniq = {}
    for it in items:
        iid = it.get("id") or hashlib.md5(it["url"].encode("utf-8")).hexdigest()
        if iid not in uniq:
            uniq[iid] = it
    items = list(uniq.values())

    # ä¾ç™¼ä½ˆæ™‚é–“é™åºï¼ˆæœ‰å›é€€ï¼‰
    items.sort(key=_published_key, reverse=True)

    # åªè¼¸å‡ºæœ€æ–° MAX_OUTPUT æ¢
    items = items[:MAX_OUTPUT]

    print(f"[DONE] output {len(items)} items")

    # stdout è¼¸å‡º JSONï¼ˆä¾› pipeline ä½¿ç”¨ï¼‰
    print(json.dumps(items, ensure_ascii=False))

if __name__ == "__main__":
    main()
